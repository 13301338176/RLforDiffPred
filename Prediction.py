## Please check readme section on github for replication steps for the method described in the paper:
## Gursoy, F., & Durahim, A. O. (2018). Predicting Diffusion Reach Probabilities via Representation Learning on Social Networks. Proceedings of the 5th International Management Information Systems Conference. doi:10.6084/m9.figshare.7565894


"""
@author: Furkan Gursoy (http://furkangursoy.github.io)
"""


import datetime
import pickle
import pandas as pd
import numpy as np
import random

### INPUTs ###
n = 1005 #number of nodes in the graph
r = 20 #number of cascades starting from each node
dimension = 128 #number of dimension/features for the node embeddings
trainingsizemultiplier = 0.1 #the portion of cascades that is available (that we are going to learn from)
cascadefilename = 'cascades_email' #name of the cascades output file generated by CascadeGenerator.py
embfile = "embeddings_email.csv" #The embeddings generated by node2vec or other methods
outputfile = "results.txt" #name of the file for storing results
########################



### READ CASCADE FILE AND CALCULATE ACTUAL VALUES###
print("begin_app", datetime.datetime.now().time())

with open (cascadefilename, 'rb') as fp:
    cascades = pickle.load(fp)
        
print("begin_Matrix", datetime.datetime.now().time())
Matrix = [[0 for x in range(n)] for y in range(n)]

for i in range(n*r):
    for j in range (len(cascades[i])):
        for k in range(len(cascades[i][j])):
            Matrix[cascades[i][0][0]] [cascades[i][j][k]] += 1           #nodes have to be from 0 to n for this to work. largest connected component etc. will break this rule.
print("end_Matrix", datetime.datetime.now().time())     #Matrix stores counts which are equivelant to actual probabilities (actual values) when normalized.

actualvalues = [0.0 for x in range(n*n)]
for i in range(n):
    for j in range(n):
        actualvalues[i*n+j] = Matrix[i][j]/r  

del Matrix
deletedx =[] #to be deleted rows ixi
for i in range(n):
    deletedx.append(i*n+i)

actualvalues = np.delete(actualvalues, deletedx, 0)
#################################



### FIND DIFFUSION PROBABILITIES BASED ON PORTION OF CASCADES (AVAILABLE CASCADES) ###
samplesize= int(len(cascades)*trainingsizemultiplier)
cascades2 = random.sample(cascades, samplesize)
Matrix2 = [[0 for x in range(n)] for y in range(n)]

for i in range(len(cascades2)):
    for j in range(len(cascades2[i])):
        for k in range(len(cascades2[i][j])):
            Matrix2[cascades2[i][0][0]] [cascades2[i][j][k]] += 1            #nodes have to be from 0 to n for this to work. largest connected component etc. will break this rule.
print("end_Matrix2", datetime.datetime.now().time())      #Matrix2 stores counts which are equivelant to label probabilities when normalized.
#############################################################

            
### READ EMBEDDINGS AND ASSIGN PROBABILITIES BASED ON PORTION OF CASCADES (AVAILABLE CASCADES) ###
print("begin_embed", datetime.datetime.now().time())

nemb = pd.read_csv(embfile, delimiter=" ", header=None).values #node embeddings
nemb = nemb[nemb[:,0].argsort()] #nodes have to be from 0 to n for this to work
lemb = np.asarray([[0.0 for x in range(len(nemb[0])*2-1)] for y in range(n*n)]) #link embeddings

#count cascades per node
ccpn = [0.01 for x in range(n)]
for i in range(len(cascades2)):
    ccpn[int(cascades2[i][0][0])] += 1

del cascades2

for i in range(len(nemb)):
    for j in range(len(nemb)):
        for l in range(len(nemb[0])-1):
            lemb[i*n+j][l] = nemb[i][l+1]
        for m in range(len(nemb[0])-1):
            lemb[i*n+j][l+1+m] = nemb[j][m+1]
        lemb[i*n+j][l+m+2] = Matrix2[i][j]/ccpn[i]

del Matrix2        
print("end_embed", datetime.datetime.now().time())

    
print("begin_delete", datetime.datetime.now().time())
deleted =[] #to be deleted rows ixi
for i in range(len(nemb)):
    deleted.append(i*n+i)

del nemb
lemb = np.delete(lemb, deleted, 0)
print("end_delete", datetime.datetime.now().time())
###############################################################


### TRAIN THE MODEL ON EMBEDDINGS AND PROBABILITIES FROM AVAILABLE CASCADES and CALCULATE ACCURACY BASED ON ACTUAL VALUES ###
print("begin_prediction", datetime.datetime.now().time())
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_absolute_error

algs = [GradientBoostingRegressor(), MLPRegressor()] #list of predictive models to be used
accuracy = []

benchmark = mean_absolute_error(actualvalues, np.repeat(np.mean(lemb[:,dimension*2]),len(lemb[:,dimension*2]))) #this is a more naive benchmark where mean is used as predictions



##benchmark calculation:
benchmark2 = mean_absolute_error(actualvalues, lemb[:,dimension*2]) 


accuracy.append(benchmark)
accuracy.append(benchmark2)

for pred in algs: #train the models and predict and calculate scores
    pred.fit(lemb[:,0:dimension*2], lemb[:,dimension*2])
    preds = pred.predict(lemb[:,0:dimension*2])   
    overall_error= mean_absolute_error(actualvalues, preds)
    accuracy.append(overall_error)
    print("end_prediction", datetime.datetime.now().time())


file = open(outputfile,"w") #mae scores of the employed models
for item in accuracy:
  file.write("%s\n" % item)
file.close() 

file = open(outputfile + "_preds","w")  #predicted values
for item in preds:
  file.write("%s\n" % item)
file.close() 

file = open(outputfile + "_actualvalues","w") #qactual values
for item in actualvalues:
  file.write("%s\n" % item)
file.close() 

# END APPLICATION